{
    "additionDate": "2020-01-14T09:13:51Z",
    "biotoolsCURIE": "biotools:ADS",
    "biotoolsID": "ADS",
    "confidence_flag": "tool",
    "credit": [
        {
            "email": "petri.toronen@helsinki.fi",
            "name": "Petri Törönen",
            "orcidid": "https://orcid.org/0000-0003-4764-9790",
            "typeEntity": "Person"
        }
    ],
    "description": "A novel comparison of evaluation metrics for gene ontology classifiers reveals drastic performance differences.",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Ontology comparison",
                    "uri": "http://edamontology.org/operation_3352"
                },
                {
                    "term": "Scatter plot plotting",
                    "uri": "http://edamontology.org/operation_2940"
                },
                {
                    "term": "Text annotation",
                    "uri": "http://edamontology.org/operation_3778"
                }
            ]
        }
    ],
    "homepage": "http://ekhidna2.biocenter.helsinki.fi/ADS/",
    "language": [
        "C++",
        "Perl",
        "R"
    ],
    "lastUpdate": "2021-01-14T15:05:09Z",
    "link": [
        {
            "type": [
                "Repository"
            ],
            "url": "https://bitbucket.org/plyusnin/ads/"
        }
    ],
    "name": "ADS",
    "owner": "Pub2Tools",
    "publication": [
        {
            "doi": "10.1371/JOURNAL.PCBI.1007419",
            "metadata": {
                "abstract": "© 2019 Plyusnin et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Automated protein annotation using the Gene Ontology (GO) plays an important role in the biosciences. Evaluation has always been considered central to developing novel annotation methods, but little attention has been paid to the evaluation metrics themselves. Evaluation metrics define how well an annotation method performs and allows for them to be ranked against one another. Unfortunately, most of these metrics were adopted from the machine learning literature without establishing whether they were appropriate for GO annotations. We propose a novel approach for comparing GO evaluation metrics called Artificial Dilution Series (ADS). Our approach uses existing annotation data to generate a series of annotation sets with different levels of correctness (referred to as their signal level). We calculate the evaluation metric being tested for each annotation set in the series, allowing us to identify whether it can separate different signal levels. Finally, we contrast these results with several false positive annotation sets, which are designed to expose systematic weaknesses in GO assessment. We compared 37 evaluation metrics for GO annotation using ADS and identified drastic differences between metrics. We show that some metrics struggle to differentiate between different signal levels, while others give erroneously high scores to the false positive data sets. Based on our findings, we provide guidelines on which evaluation metrics perform well with the Gene Ontology and propose improvements to several well-known evaluation metrics. In general, we argue that evaluation metrics should be tested for their performance and we provide software for this purpose (https://bitbucket.org/plyusnin/ads/). ADS is applicable to other areas of science where the evaluation of prediction results is nontrivial.",
                "authors": [
                    {
                        "name": "Holm L."
                    },
                    {
                        "name": "Plyusnin I."
                    },
                    {
                        "name": "Toronen P."
                    }
                ],
                "citationCount": 3,
                "date": "2019-01-01T00:00:00Z",
                "journal": "PLoS Computational Biology",
                "title": "Novel comparison of evaluation metrics for gene ontology classifiers reveals drastic performance differences"
            },
            "pmcid": "PMC6855565",
            "pmid": "31682632"
        }
    ],
    "toolType": [
        "Workflow"
    ],
    "topic": [
        {
            "term": "Bioinformatics",
            "uri": "http://edamontology.org/topic_0091"
        },
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Ontology and terminology",
            "uri": "http://edamontology.org/topic_0089"
        }
    ],
    "validated": 1
}
