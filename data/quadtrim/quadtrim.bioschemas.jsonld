{
  "@context": {
    "biotools": "https://bio.tools/ontology/",
    "brick": "https://brickschema.org/schema/Brick#",
    "bsc": "http://bioschemas.org/",
    "bsct": "http://bioschemas.org/types/",
    "csvw": "http://www.w3.org/ns/csvw#",
    "dc": "http://purl.org/dc/elements/1.1/",
    "dcam": "http://purl.org/dc/dcam/",
    "dcat": "http://www.w3.org/ns/dcat#",
    "dcmitype": "http://purl.org/dc/dcmitype/",
    "dcterms": "http://purl.org/dc/terms/",
    "doap": "http://usefulinc.com/ns/doap#",
    "edam": "http://edamontology.org/",
    "foaf": "http://xmlns.com/foaf/0.1/",
    "geo": "http://www.opengis.net/ont/geosparql#",
    "odrl": "http://www.w3.org/ns/odrl/2/",
    "org": "http://www.w3.org/ns/org#",
    "owl": "http://www.w3.org/2002/07/owl#",
    "prof": "http://www.w3.org/ns/dx/prof/",
    "prov": "http://www.w3.org/ns/prov#",
    "qb": "http://purl.org/linked-data/cube#",
    "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
    "sc": "http://schema.org/",
    "schema": "https://schema.org/",
    "sh": "http://www.w3.org/ns/shacl#",
    "skos": "http://www.w3.org/2004/02/skos/core#",
    "sosa": "http://www.w3.org/ns/sosa/",
    "ssn": "http://www.w3.org/ns/ssn/",
    "time": "http://www.w3.org/2006/time#",
    "vann": "http://purl.org/vocab/vann/",
    "void": "http://rdfs.org/ns/void#",
    "wgs": "https://www.w3.org/2003/01/geo/wgs84_pos#",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
  },
  "@graph": [
    {
      "@id": "https://bio.tools/QuAdTrim",
      "@type": "sc:SoftwareApplication",
      "dcterms:conformsTo": "https://bioschemas.org/profiles/ComputationalTool/0.6-DRAFT",
      "sc:applicationSubCategory": {
        "@id": "edam:topic_3168"
      },
      "sc:citation": {
        "@id": "https://doi.org/10.1101/2019.12.18.870642"
      },
      "sc:description": "Overcoming computational bottlenecks in sequence quality control.\n\nAbstract With the recent torrent of high throughput sequencing (HTS) data the necessity for highly efficient algorithms for common tasks is paramount. One task for which the basis for all further analysis of HTS data is initial data quality control, that is, the removal or trimming of poor quality reads from the dataset. Here we present QuAdTrim, a quality control and adapter trimming algorithm for HTS data that is up to 57 times faster and uses less than 0.06% of the memory of other commonly used HTS quality control programs. QuAdTrim will reduce the time and memory required for quality control of HTS data, and in doing, will reduce the computational demands of a fundamental step in HTS data analysis. Additionally, QuAdTrim impliments the removal of homopolymer Gs from the 3â€™ end of sequence reads, a common error generated on the NovaSeq, NextSeq and iSeq100 platforms",
      "sc:featureList": [
        {
          "@id": "edam:operation_3192"
        },
        {
          "@id": "edam:operation_3218"
        },
        {
          "@id": "edam:operation_3237"
        }
      ],
      "sc:license": "BSD-3-Clause",
      "sc:name": "QuAdTrim",
      "sc:url": "https://bitbucket.org/arobinson/quadtrim"
    },
    {
      "@id": "https://doi.org/10.1101/2019.12.18.870642",
      "@type": "sc:CreativeWork"
    }
  ]
}