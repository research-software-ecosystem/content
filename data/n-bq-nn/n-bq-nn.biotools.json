{
    "additionDate": "2021-01-18T08:46:36Z",
    "biotoolsCURIE": "biotools:n-bq-nn",
    "biotoolsID": "n-bq-nn",
    "confidence_flag": "tool",
    "description": "A Learning Framework for n-Bit Quantized Neural Networks Toward FPGAs.\n\nThis implementation supports cifar10/cifar100/imagenet.",
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Standardisation and normalisation",
                    "uri": "http://edamontology.org/operation_3435"
                }
            ]
        }
    ],
    "homepage": "http://github.com/papcjy/n-BQ-NN",
    "language": [
        "Python"
    ],
    "lastUpdate": "2021-03-08T08:27:30Z",
    "name": "n-BQ-NN",
    "owner": "Niclaskn",
    "publication": [
        {
            "doi": "10.1109/TNNLS.2020.2980041",
            "metadata": {
                "abstract": "\u00a9 2012 IEEE.The quantized neural network (QNN) is an efficient approach for network compression and can be widely used in the implementation of field-programmable gate arrays (FPGAs). This article proposes a novel learning framework for $n$ -bit QNNs, whose weights are constrained to the power of two. To solve the gradient vanishing problem, we propose a reconstructed gradient function for QNNs in the back-propagation algorithm that can directly get the real gradient rather than estimating an approximate gradient of the expected loss. We also propose a novel QNN structure named $n$ -BQ-NN, which uses shift operation to replace the multiply operation and is more suitable for the inference on FPGAs. Furthermore, we also design a shift vector processing element (SVPE) array to replace all 16-bit multiplications with SHIFT operations in convolution operation on FPGAs. We also carry out comparable experiments to evaluate our framework. The experimental results show that the quantized models of ResNet, DenseNet, and AlexNet through our learning framework can achieve almost the same accuracies with the original full-precision models. Moreover, when using our learning framework to train our $n$ -BQ-NN from scratch, it can achieve state-of-the-art results compared with typical low-precision QNNs. Experiments on Xilinx ZCU102 platform show that our $n$ -BQ-NN with our SVPE can execute 2.9 times faster than that with the vector processing element (VPE) in inference. As the SHIFT operation in our SVPE array will not consume digital signal processing (DSP) resources on FPGAs, the experiments have shown that the use of SVPE array also reduces average energy consumption to 68.7% of the VPE array with 16 bit.",
                "authors": [
                    {
                        "name": "Chen J."
                    },
                    {
                        "name": "Liu L."
                    },
                    {
                        "name": "Liu Y."
                    },
                    {
                        "name": "Zeng X."
                    }
                ],
                "date": "2021-03-01T00:00:00Z",
                "journal": "IEEE Transactions on Neural Networks and Learning Systems",
                "title": "A Learning Framework for n-Bit Quantized Neural Networks Toward FPGAs"
            },
            "pmid": "32287015"
        }
    ],
    "toolType": [
        "Command-line tool"
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        },
        {
            "term": "Transcription factors and regulatory sites",
            "uri": "http://edamontology.org/topic_0749"
        }
    ]
}