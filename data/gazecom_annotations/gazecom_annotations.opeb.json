{"credit":[],"function":[{"operation":[{"term":"Quantification","uri":"http://edamontology.org/operation_3799"},{"term":"Essential dynamics","uri":"http://edamontology.org/operation_3891"},{"term":"Visualisation","uri":"http://edamontology.org/operation_0337"}]}],"labels":{"topic":[{"term":"Opthalmology","uri":"http://edamontology.org/topic_3417"},{"term":"Machine learning","uri":"http://edamontology.org/topic_3474"},{"term":"Imaging","uri":"http://edamontology.org/topic_3382"},{"term":"Ecology","uri":"http://edamontology.org/topic_0610"}]},"publication":[{"abstract":"Eye movements are fundamental to our visual experience of the real world, and tracking smooth pursuit eye movements play an important role because of the dynamic nature of our environment. Static images, however, do not induce this class of eye movements, and commonly used synthetic moving stimuli lack ecological validity because of their low scene complexity compared to the real world. Traditionally, ground truth data for pursuit analyses with naturalistic stimuli are obtained via laborious hand-labelling. Therefore, previous studies typically remained small in scale. We here present the first large-scale quantitative characterization of human smooth pursuit. In order to achieve this, we first provide a methodological framework for such analyses by collecting a large set of manual annotations for eye movements in dynamic scenes and by examining the bias and variance of human annotators. To enable further research on even larger future data sets, we also describe, improve, and thoroughly analyze a novel algorithm to automatically classify eye movements. Our approach incorporates unsupervised learning techniques and thus demonstrates improved performance with the addition of unlabelled data. The code and data related to our manual and automated eye movement annotation are publicly available via https://web.gin.g-node.org/ioannis.agtzidis/gazecom_annotations/.","authors":["Startsev M.","Agtzidis I.","Dorr M."],"cit_count":0,"doi":"10.1167/19.14.10","journal":"Journal of vision","pmid":"31830239","title":"Characterizing and automatically detecting smooth pursuit in a large-scale ground-truth data set of dynamic natural scenes","year":"2019-12-02"}],"summary":{"biotoolsCURIE":"biotools:gazecom_annotations","biotoolsID":"gazecom_annotations","description":"Characterizing and automatically detecting smooth pursuit in a large-scale ground-truth data set of dynamic natural scenes.\n\nHand labelled eye movements and targets, together with the output of three automatic classification algorithms for the GazeCom data set","homepage":"https://web.gin.g-node.org/ioannis.agtzidis/gazecom_annotations/","name":"gazecom_annotations"}}