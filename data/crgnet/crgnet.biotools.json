{
    "accessibility": "Open access",
    "additionDate": "2022-10-03T08:56:44.832711Z",
    "biotoolsCURIE": "biotools:crgnet",
    "biotoolsID": "crgnet",
    "confidence_flag": "tool",
    "cost": "Free of charge",
    "credit": [
        {
            "name": "Pedram Ghamisi",
            "orcidid": "https://orcid.org/0000-0003-1203-741X"
        },
        {
            "name": "Yonghao Xu",
            "orcidid": "https://orcid.org/0000-0002-6857-0152"
        }
    ],
    "description": "Consistency-regularized region-growing network (CRGNet) for semantic segmentation of VHR remote sensing images using point-level annotations. The key idea of CRGNet is to iteratively select unlabeled pixels with high confidence to expand the annotated area from the original sparse points",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Image analysis",
                    "uri": "http://edamontology.org/operation_3443"
                },
                {
                    "term": "Image annotation",
                    "uri": "http://edamontology.org/operation_3553"
                }
            ]
        }
    ],
    "homepage": "https://github.com/YonghaoXu/CRGNet",
    "language": [
        "Python"
    ],
    "lastUpdate": "2022-10-03T08:56:44.835242Z",
    "license": "MIT",
    "name": "CRGNet",
    "owner": "Chan019",
    "publication": [
        {
            "doi": "10.1109/TIP.2022.3189825",
            "metadata": {
                "abstract": "Â© 1992-2012 IEEE.Deep learning algorithms have obtained great success in semantic segmentation of very high-resolution (VHR) remote sensing images. Nevertheless, training these models generally requires a large amount of accurate pixel-wise annotations, which is very laborious and time-consuming to collect. To reduce the annotation burden, this paper proposes a consistency-regularized region-growing network (CRGNet) to achieve semantic segmentation of VHR remote sensing images with point-level annotations. The key idea of CRGNet is to iteratively select unlabeled pixels with high confidence to expand the annotated area from the original sparse points. However, since there may exist some errors and noises in the expanded annotations, directly learning from them may mislead the training of the network. To this end, we further propose the consistency regularization strategy, where a base classifier and an expanded classifier are employed. Specifically, the base classifier is supervised by the original sparse annotations, while the expanded classifier aims to learn from the expanded annotations generated by the base classifier with the region-growing mechanism. The consistency regularization is thereby achieved by minimizing the discrepancy between the predictions from both the base and the expanded classifiers. We find such a simple regularization strategy is yet very useful to control the quality of the region-growing mechanism. Extensive experiments on two benchmark datasets demonstrate that the proposed CRGNet significantly outperforms the existing state-of-the-art methods. Codes and pre-trained models are available online (https://github.com/YonghaoXu/CRGNet).",
                "authors": [
                    {
                        "name": "Ghamisi P."
                    },
                    {
                        "name": "Xu Y."
                    }
                ],
                "date": "2022-01-01T00:00:00Z",
                "journal": "IEEE Transactions on Image Processing",
                "title": "Consistency-Regularized Region-Growing Network for Semantic Segmentation of Urban Scenes with Point-Level Annotations"
            },
            "pmid": "35877807"
        }
    ],
    "toolType": [
        "Script"
    ],
    "topic": [
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        },
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        }
    ]
}
