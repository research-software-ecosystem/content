{
    "additionDate": "2021-01-18T12:24:59Z",
    "biotoolsCURIE": "biotools:enaet",
    "biotoolsID": "enaet",
    "confidence_flag": "tool",
    "description": "EnAET: Self-Trained Ensemble AutoEncoding Transformations forSemi-Supervised Learning.\n\nDeep neural networks have been successfully applied to many real-world applications.",
    "editPermission": {
        "type": "private"
    },
    "homepage": "https://github.com/maple-research-lab/EnAET",
    "language": [
        "Python",
        "Maple"
    ],
    "lastUpdate": "2021-03-07T10:46:12Z",
    "license": "MIT",
    "name": "EnAET",
    "owner": "zsmag19",
    "publication": [
        {
            "doi": "10.1109/TIP.2020.3044220",
            "metadata": {
                "abstract": "\u00a9 1992-2012 IEEE.Deep neural networks have been successfully applied to many real-world applications. However, such successes rely heavily on large amounts of labeled data that is expensive to obtain. Recently, many methods for semi-supervised learning have been proposed and achieved excellent performance. In this study, we propose a new EnAET framework to further improve existing semi-supervised methods with self-supervised information. To our best knowledge, all current semi-supervised methods improve performance with prediction consistency and confidence ideas. We are the first to explore the role of self-supervised representations in semi-supervised learning under a rich family of transformations. Consequently, our framework can integrate the self-supervised information as a regularization term to further improve all current semi-supervised methods. In the experiments, we use MixMatch, which is the current state-of-The-Art method on semi-supervised learning, as a baseline to test the proposed EnAET framework. Across different datasets, we adopt the same hyper-parameters, which greatly improves the generalization ability of the EnAET framework. Experiment results on different datasets demonstrate that the proposed EnAET framework greatly improves the performance of current semi-supervised algorithms. Moreover, this framework can also improve supervised learning by a large margin, including the extremely challenging scenarios with only 10 images per class. The code and experiment records are available in https://github.com/maple-research-lab/EnAET.",
                "authors": [
                    {
                        "name": "Wang X."
                    },
                    {
                        "name": "Kihara D."
                    },
                    {
                        "name": "Luo J."
                    },
                    {
                        "name": "Qi G.-J."
                    }
                ],
                "citationCount": 3,
                "date": "2021-01-01T00:00:00Z",
                "journal": "IEEE Transactions on Image Processing",
                "title": "EnAET: A Self-Trained Framework for Semi-Supervised and Supervised Learning with Ensemble Transformations"
            },
            "pmid": "33347409"
        }
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        }
    ]
}