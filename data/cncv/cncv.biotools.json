{
    "additionDate": "2021-01-18T07:01:58Z",
    "biotoolsCURIE": "biotools:cncv",
    "biotoolsID": "cncv",
    "confidence_flag": "tool",
    "credit": [
        {
            "email": "brett.mckinney@utulsa.edu",
            "name": "Brett A McKinney",
            "typeEntity": "Person"
        }
    ],
    "description": "cnCV is an R package for Consensus Nested Cross-Validation.",
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Feature selection",
                    "uri": "http://edamontology.org/operation_3936"
                },
                {
                    "term": "Standardisation and normalisation",
                    "uri": "http://edamontology.org/operation_3435"
                }
            ]
        }
    ],
    "homepage": "https://github.com/insilico/cncv",
    "language": [
        "R"
    ],
    "lastUpdate": "2021-02-13T19:38:48Z",
    "name": "cnCV",
    "owner": "Kigaard",
    "publication": [
        {
            "doi": "10.1093/BIOINFORMATICS/BTAA046",
            "metadata": {
                "abstract": "\u00a9 2020 The Author(s). Published by Oxford University Press. All rights reserved.Feature selection can improve the accuracy of machine-learning models, but appropriate steps must be taken to avoid overfitting. Nested cross-validation (nCV) is a common approach that chooses the classification model and features to represent a given outer fold based on features that give the maximum inner-fold accuracy. Differential privacy is a related technique to avoid overfitting that uses a privacy-preserving noise mechanism to identify features that are stable between training and holdout sets. We develop consensus nested cross-validation (cnCV) that combines the idea of feature stability from differential privacy with nCV. Feature selection is applied in each inner fold and the consensus of top features across folds is used as a measure of feature stability or reliability instead of classification accuracy, which is used in standard nCV. We use simulated data with main effects, correlation and interactions to compare the classification accuracy and feature selection performance of the new cnCV with standard nCV, Elastic Net optimized by cross-validation, differential privacy and private evaporative cooling (pEC). We also compare these methods using real RNA-seq data from a study of major depressive disorder. The cnCV method has similar training and validation accuracy to nCV, but cnCV has much shorter run times because it does not construct classifiers in the inner folds. The cnCV method chooses a more parsimonious set of features with fewer false positives than nCV. The cnCV method has similar accuracy to pEC and cnCV selects stable features between folds without the need to specify a privacy threshold. We show that cnCV is an effective and efficient approach for combining feature selection with classification.",
                "authors": [
                    {
                        "name": "Parvandeh S."
                    },
                    {
                        "name": "Yeh H.-W."
                    },
                    {
                        "name": "Paulus M.P."
                    },
                    {
                        "name": "McKinney B.A."
                    }
                ],
                "citationCount": 4,
                "date": "2020-05-01T00:00:00Z",
                "journal": "Bioinformatics",
                "title": "Consensus features nested cross-validation"
            },
            "pmid": "31985777"
        },
        {
            "doi": "10.1101/2019.12.31.891895"
        }
    ],
    "toolType": [
        "Library"
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "RNA-Seq",
            "uri": "http://edamontology.org/topic_3170"
        },
        {
            "term": "Data security",
            "uri": "http://edamontology.org/topic_3263"
        }
    ]
}