
{
    "credit": [
    ],
    "labels": {
        "topic": [
            {
                "term": "Laboratory techniques",
                "uri": "http://edamontology.org/topic_3361"
            }
        ]
    },
    "publication": [
        {
            "doi": "10.1109/TCYB.2019.2957101",
            "pmid": "31880577"
        }
    ],
    "summary": {
        "biotoolsCURIE": "biotools:SPLBoost",
        "biotoolsID": "SPLBoost",
        "description": "An Improved Robust Boosting Algorithm Based on Self-Paced Learning.\n\nIt is known that boosting can be interpreted as an optimization technique to minimize an underlying loss function. Specifically, the underlying loss being minimized by the traditional AdaBoost is the exponential loss, which proves to be very sensitive to random noise outliers. Therefore, several boosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to improve the robustness of AdaBoost by replacing the exponential loss with some designed robust loss functions. In this article, we present a new way to robustify AdaBoost, that is, incorporating the robust learning idea of self-paced learning (SPL) into the boosting framework. Specifically, we design a new robust boosting algorithm based on the SPL regime, that is, SPLBoost, which can be easily implemented by slightly modifying off-the-shelf boosting packages.\n\n||| HOMEPAGE MISSING!",
        "homepage": "https://www.ncbi.nlm.nih.gov/pubmed/?term=31880577",
        "name": "SPLBoost"
    }
}