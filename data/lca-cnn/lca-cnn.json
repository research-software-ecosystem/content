{
    "additionDate": "2020-01-14T09:49:11Z",
    "biotoolsCURIE": "biotools:LCA-CNN",
    "biotoolsID": "LCA-CNN",
    "confidence_flag": "very low",
    "description": "Learning Cascade Attention for fine-grained image classification.\n\nCode for paper \"Learning Cascade Attention for Fine-grained Image Classification\", which currently under review at Elsevier Journal of Neural Networks(NN).\n\nTrain/test split for CUB-200-2011 dataset.\n\nRotate training set for data augmentation.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'Cross-network', 'pre-train', 'CUB-200-2011', 'FGVC-Aircraft'",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Splitting",
                    "uri": "http://edamontology.org/operation_3359"
                }
            ]
        }
    ],
    "homepage": "https://github.com/billzyx/LCA-CNN",
    "language": [
        "Python"
    ],
    "lastUpdate": "2020-01-14T09:49:11Z",
    "name": "LCA-CNN",
    "owner": "Pub2Tools",
    "publication": [
        {
            "doi": "10.1016/J.NEUNET.2019.10.009",
            "metadata": {
                "abstract": "\u00a9 2019Fine-grained image classification is a challenging task due to the large inter-class difference and small intra-class difference. In this paper, we propose a novel Cascade Attention Model using the Deep Convolutional Neural Network to address this problem. Our method first leverages the Spatial Confusion Attention to identify ambiguous areas of the input image. Two constraint loss functions are proposed: the Spatial Mask loss and the Spatial And loss; Second, the Cross-network Attention, applying different pre-train parameters to the two stream architecture. Also, two novel loss functions called Cross-network Similarity loss and Satisfied Rank loss are proposed to make the two-stream networks reinforce each other and get better results. Finally, the Network Fusion Attention merges intermediate results with the novel entropy add strategy to obtain the final predictions. All of these modules can work together and can be trained end to end. Besides, different from previous works, our model is fully weak-supervised and fully paralleled, which leads to easier generalization and faster computation. We obtain the state-of-the-art performance on three challenge benchmark datasets (CUB-200-2011, FGVC-Aircraft and Flower 102) with results of 90.8%, 92.1%, and 98.5%, respectively. The model will be publicly available at https://github.com/billzyx/LCA-CNN.",
                "authors": [
                    {
                        "name": "Zhu Y."
                    },
                    {
                        "name": "Li R."
                    },
                    {
                        "name": "Yang Y."
                    },
                    {
                        "name": "Ye N."
                    }
                ],
                "date": "2020-02-01T00:00:00Z",
                "journal": "Neural Networks",
                "title": "Learning Cascade Attention for fine-grained image classification"
            },
            "pmid": "31683145"
        }
    ],
    "topic": [
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        },
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        }
    ]
}