{
    "accessibility": "Open access",
    "additionDate": "2022-06-08T13:31:48.819387Z",
    "biotoolsCURIE": "biotools:utrad",
    "biotoolsID": "utrad",
    "confidence_flag": "tool",
    "cost": "Free of charge",
    "credit": [
        {
            "name": "Juntong Xi"
        },
        {
            "name": "Liyang Chen"
        },
        {
            "name": "Zhiyuan You"
        },
        {
            "name": "Nian Zhang",
            "orcidid": "https://orcid.org/0000-0003-1916-7719"
        },
        {
            "name": "Xinyi Le",
            "orcidid": "https://orcid.org/0000-0003-0318-9497"
        }
    ],
    "description": "Anomaly detection and localization with U-Transformer.",
    "editPermission": {
        "type": "private"
    },
    "homepage": "https://github.com/gordon-chenmo/UTRAD",
    "language": [
        "Python"
    ],
    "lastUpdate": "2022-06-08T13:31:48.822008Z",
    "license": "Not licensed",
    "name": "UTRAD",
    "operatingSystem": [
        "Linux",
        "Mac",
        "Windows"
    ],
    "owner": "Jennifer",
    "publication": [
        {
            "doi": "10.1016/J.NEUNET.2021.12.008",
            "metadata": {
                "abstract": "Â© 2021Anomaly detection is an active research field in industrial defect detection and medical disease detection. However, previous anomaly detection works suffer from unstable training, or non-universal criteria of evaluating feature distribution. In this paper, we introduce UTRAD, a U-TRansformer based Anomaly Detection framework. Deep pre-trained features are regarded as dispersed word tokens, and represented with transformer-based autoencoders. With reconstruction on more informative feature distribution instead of raw images, we achieve a more stable training process and a more precise anomaly detection and localization result. In addition, our proposed UTRAD has a multi-scale pyramidal hierarchy with skip connections that help detect both multi-scale structural and non-structural anomalies. As attention layers are decomposed to multi-level patches, UTRAD significantly reduces the computational cost and memory usage compared with the vanilla transformer. Experiments on industrial dataset MVtec AD and medical datasets Retinal-OCT, Brain-MRI, Head-CT have been conducted. Our proposed UTRAD out-performs all other state-of-the-art methods in the above datasets. Code released at https://github.com/gordon-chenmo/UTRAD.",
                "authors": [
                    {
                        "name": "Chen L."
                    },
                    {
                        "name": "Le X."
                    },
                    {
                        "name": "Xi J."
                    },
                    {
                        "name": "You Z."
                    },
                    {
                        "name": "Zhang N."
                    }
                ],
                "citationCount": 1,
                "date": "2022-03-01T00:00:00Z",
                "journal": "Neural Networks",
                "title": "UTRAD: Anomaly detection and localization with U-Transformer"
            },
            "pmid": "34973607"
        }
    ],
    "toolType": [
        "Command-line tool"
    ],
    "topic": [
        {
            "term": "MRI",
            "uri": "http://edamontology.org/topic_3444"
        },
        {
            "term": "Medical imaging",
            "uri": "http://edamontology.org/topic_3384"
        },
        {
            "term": "Tomography",
            "uri": "http://edamontology.org/topic_3452"
        }
    ],
    "validated": 1
}
