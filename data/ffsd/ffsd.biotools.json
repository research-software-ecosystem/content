{
    "additionDate": "2022-06-25T14:15:26.337147Z",
    "biotoolsCURIE": "biotools:ffsd",
    "biotoolsID": "ffsd",
    "confidence_flag": "tool",
    "credit": [
        {
            "name": "Rongrong Ji",
            "orcidid": "https://orcid.org/0000-0001-9163-2932"
        },
        {
            "name": "Shaojie Li",
            "orcidid": "https://orcid.org/0000-0002-7203-2488"
        }
    ],
    "description": "Distilling a Powerful Student Model via Online Knowledge Distillation.",
    "editPermission": {
        "type": "public"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Splitting",
                    "uri": "http://edamontology.org/operation_3359"
                }
            ]
        }
    ],
    "homepage": "https://github.com/SJLeo/FFSD",
    "language": [
        "Python"
    ],
    "lastUpdate": "2022-06-25T14:15:26.347043Z",
    "license": "Not licensed",
    "name": "FFSD",
    "operatingSystem": [
        "Linux"
    ],
    "owner": "Chan019",
    "publication": [
        {
            "doi": "10.1109/TNNLS.2022.3152732",
            "metadata": {
                "abstract": "IEEEExisting online knowledge distillation approaches either adopt the student with the best performance or construct an ensemble model for better holistic performance. However, the former strategy ignores other students' information, while the latter increases the computational complexity during deployment. In this article, we propose a novel method for online knowledge distillation, termed feature fusion and self-distillation (FFSD), which comprises two key components: FFSD, toward solving the above problems in a unified framework. Different from previous works, where all students are treated equally, the proposed FFSD splits them into a leader student set and a common student set. Then, the feature fusion module converts the concatenation of feature maps from all common students into a fused feature map. The fused representation is used to assist the learning of the leader student. To enable the leader student to absorb more diverse information, we design an enhancement strategy to increase the diversity among students. Besides, a self-distillation module is adopted to convert the feature map of deeper layers into a shallower one. Then, the shallower layers are encouraged to mimic the transformed feature maps of the deeper layers, which helps the students to generalize better. After training, we simply adopt the leader student, which achieves superior performance, over the common students, without increasing the storage or inference cost. Extensive experiments on CIFAR-100 and ImageNet demonstrate the superiority of our FFSD over existing works. The code is available at https://github.com/SJLeo/FFSD.",
                "authors": [
                    {
                        "name": "Ji R."
                    },
                    {
                        "name": "Li S."
                    },
                    {
                        "name": "Lin M."
                    },
                    {
                        "name": "Shao L."
                    },
                    {
                        "name": "Tian Y."
                    },
                    {
                        "name": "Wang Y."
                    },
                    {
                        "name": "Wu Y."
                    }
                ],
                "date": "2022-01-01T00:00:00Z",
                "journal": "IEEE Transactions on Neural Networks and Learning Systems",
                "title": "Distilling a Powerful Student Model via Online Knowledge Distillation"
            },
            "pmid": "35254994"
        }
    ],
    "toolType": [
        "Workbench"
    ],
    "topic": [
        {
            "term": "Laboratory techniques",
            "uri": "http://edamontology.org/topic_3361"
        },
        {
            "term": "Mapping",
            "uri": "http://edamontology.org/topic_0102"
        }
    ]
}
