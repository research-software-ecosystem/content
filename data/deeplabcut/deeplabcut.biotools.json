{
    "additionDate": "2024-02-20T12:57:39.400450Z",
    "biotoolsCURIE": "biotools:deeplabcut",
    "biotoolsID": "deeplabcut",
    "description": "DeepLabCut™️ is a toolbox for state-of-the-art markerless pose estimation of animals performing various behaviors.",
    "editPermission": {
        "type": "private"
    },
    "homepage": "https://github.com/DeepLabCut/DeepLabCut",
    "language": [
        "Python"
    ],
    "lastUpdate": "2024-11-24T14:14:48.368594Z",
    "license": "LGPL-3.0",
    "name": "DeepLabCut",
    "owner": "nadgoue",
    "publication": [
        {
            "doi": "10.1038/s41592-022-01443-0",
            "metadata": {
                "abstract": "Estimating the pose of multiple animals is a challenging computer vision problem: frequent interactions cause occlusions and complicate the association of detected keypoints to the correct individuals, as well as having highly similar looking animals that interact more closely than in typical multi-human scenarios. To take up this challenge, we build on DeepLabCut, an open-source pose estimation toolbox, and provide high-performance animal assembly and tracking—features required for multi-animal scenarios. Furthermore, we integrate the ability to predict an animal’s identity to assist tracking (in case of occlusions). We illustrate the power of this framework with four datasets varying in complexity, which we release to serve as a benchmark for future algorithm development.",
                "authors": [
                    {
                        "name": "Di Santo V."
                    },
                    {
                        "name": "Dulac C."
                    },
                    {
                        "name": "Feng G."
                    },
                    {
                        "name": "Lauder G."
                    },
                    {
                        "name": "Lauer J."
                    },
                    {
                        "name": "Mathis A."
                    },
                    {
                        "name": "Mathis M.W."
                    },
                    {
                        "name": "Menegas W."
                    },
                    {
                        "name": "Murthy V.N."
                    },
                    {
                        "name": "Nath T."
                    },
                    {
                        "name": "Rahman M.M."
                    },
                    {
                        "name": "Schneider S."
                    },
                    {
                        "name": "Soberanes D."
                    },
                    {
                        "name": "Ye S."
                    },
                    {
                        "name": "Zhou M."
                    }
                ],
                "citationCount": 193,
                "date": "2022-04-01T00:00:00Z",
                "journal": "Nature Methods",
                "title": "Multi-animal pose estimation, identification and tracking with DeepLabCut"
            },
            "pmcid": "PMC9007739",
            "pmid": "35414125"
        },
        {
            "doi": "10.1038/s41593-018-0209-y",
            "metadata": {
                "abstract": "Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (~200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.",
                "authors": [
                    {
                        "name": "Abe T."
                    },
                    {
                        "name": "Bethge M."
                    },
                    {
                        "name": "Cury K.M."
                    },
                    {
                        "name": "Mamidanna P."
                    },
                    {
                        "name": "Mathis A."
                    },
                    {
                        "name": "Mathis M.W."
                    },
                    {
                        "name": "Murthy V.N."
                    }
                ],
                "citationCount": 2182,
                "date": "2018-09-01T00:00:00Z",
                "journal": "Nature Neuroscience",
                "title": "DeepLabCut: markerless pose estimation of user-defined body parts with deep learning"
            },
            "pmid": "30127430"
        },
        {
            "doi": "10.1038/s41596-019-0176-0",
            "metadata": {
                "abstract": "Noninvasive behavioral tracking of animals during experiments is critical to many scientific pursuits. Extracting the poses of animals without using markers is often essential to measuring behavioral effects in biomechanics, genetics, ethology, and neuroscience. However, extracting detailed poses without markers in dynamically changing backgrounds has been challenging. We recently introduced an open-source toolbox called DeepLabCut that builds on a state-of-the-art human pose-estimation algorithm to allow a user to train a deep neural network with limited training data to precisely track user-defined features that match human labeling accuracy. Here, we provide an updated toolbox, developed as a Python package, that includes new features such as graphical user interfaces (GUIs), performance improvements, and active-learning-based network refinement. We provide a step-by-step procedure for using DeepLabCut that guides the user in creating a tailored, reusable analysis pipeline with a graphical processing unit (GPU) in 1–12 h (depending on frame size). Additionally, we provide Docker environments and Jupyter Notebooks that can be run on cloud resources such as Google Colaboratory.",
                "authors": [
                    {
                        "name": "Bethge M."
                    },
                    {
                        "name": "Chen A.C."
                    },
                    {
                        "name": "Mathis A."
                    },
                    {
                        "name": "Mathis M.W."
                    },
                    {
                        "name": "Nath T."
                    },
                    {
                        "name": "Patel A."
                    }
                ],
                "citationCount": 654,
                "date": "2019-07-01T00:00:00Z",
                "journal": "Nature Protocols",
                "title": "Using DeepLabCut for 3D markerless pose estimation across species and behaviors"
            },
            "pmid": "31227823"
        }
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Neurobiology",
            "uri": "http://edamontology.org/topic_3304"
        }
    ]
}
