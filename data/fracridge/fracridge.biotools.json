{
    "additionDate": "2020-09-23T06:05:05Z",
    "biotoolsCURIE": "biotools:fracridge",
    "biotoolsID": "fracridge",
    "credit": [
        {
            "email": "arokem@uw.edu",
            "name": "Ariel Rokem",
            "orcidid": "https://orcid.org/0000-0003-0679-1985",
            "typeEntity": "Person"
        },
        {
            "email": "kay@umn.edu",
            "name": "Kendrick Kay",
            "orcidid": "https://orcid.org/0000-0001-6604-9155",
            "typeEntity": "Person"
        }
    ],
    "description": "Ridge regression (RR) is a regularization technique that penalizes the L2-norm of the coefficients in linear regression. One of the challenges of using RR is the need to set a hyperparameter (\u03b1) that controls the amount of regularization. Cross-validation is typically used to select the best \u03b1 from a set of candidates. However, efficient and appropriate selection of \u03b1 can be challenging, particularly where large amounts of data are analyzed. Because the selected \u03b1 depends on the scale of the data and predictors, it is also not straightforwardly interpretable.\n\nIn fracridge, we reparameterize RR in terms of the ratio \u03b3 between the L2-norms of the regularized and unregularized coefficients. This approach, called fractional RR (FRR), has several benefits: the solutions obtained for different \u03b3 are guaranteed to vary, guarding against wasted calculations, and automatically span the relevant range of regularization, avoiding the need for arduous manual exploration.",
    "download": [
        {
            "type": "Software package",
            "url": "https://github.com/nrdg/fracridge/releases/tag/1.2"
        }
    ],
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Regression analysis",
                    "uri": "http://edamontology.org/operation_3659"
                },
                {
                    "term": "Standardisation and normalisation",
                    "uri": "http://edamontology.org/operation_3435"
                }
            ]
        }
    ],
    "homepage": "https://nrdg.github.io/fracridge/",
    "language": [
        "Python",
        "MATLAB"
    ],
    "lastUpdate": "2021-03-11T11:01:49Z",
    "license": "BSD-2-Clause",
    "link": [
        {
            "type": [
                "Other"
            ],
            "url": "https://nrdg.github.io/fracridge"
        }
    ],
    "name": "fracridge",
    "owner": "arokem",
    "publication": [
        {
            "doi": "10.1093/GIGASCIENCE/GIAA133",
            "metadata": {
                "abstract": "\u00a9 2020 The Author(s).Background: Ridge regression is a regularization technique that penalizes the L2-norm of the coefficients in linear regression. One of the challenges of using ridge regression is the need to set a hyperparameter (\u03b1) that controls the amount of regularization. Cross-validation is typically used to select the best \u03b1 from a set of candidates. However, efficient and appropriate selection of \u03b1 can be challenging. This becomes prohibitive when large amounts of data are analyzed. Because the selected \u03b1 depends on the scale of the data and correlations across predictors, it is also not straightforwardly interpretable. Results: The present work addresses these challenges through a novel approach to ridge regression. We propose to reparameterize ridge regression in terms of the ratio \u03b3 between the L2-norms of the regularized and unregularized coefficients. We provide an algorithm that efficiently implements this approach, called fractional ridge regression, as well as open-source software implementations in Python and MATLAB (https://github.com/nrdg/fracridge). We show that the proposed method is fast and scalable for large-scale data problems. In brain imaging data, we demonstrate that this approach delivers results that are straightforward to interpret and compare across models and datasets. Conclusion: Fractional ridge regression has several benefits: the solutions obtained for different \u03b3 are guaranteed to vary, guarding against wasted calculations; and automatically span the relevant range of regularization, avoiding the need for arduous manual exploration. These properties make fractional ridge regression particularly suitable for analysis of large complex datasets.",
                "authors": [
                    {
                        "name": "Rokem A."
                    },
                    {
                        "name": "Kay K."
                    }
                ],
                "date": "2021-12-01T00:00:00Z",
                "journal": "GigaScience",
                "title": "Fractional ridge regression: A fast, interpretable reparameterization of ridge regression"
            },
            "pmcid": "PMC7702219",
            "pmid": "33252656"
        }
    ],
    "topic": [
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        }
    ],
    "validated": 1,
    "version": [
        "1.2.1"
    ]
}