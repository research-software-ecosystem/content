{
    "additionDate": "2021-01-18T09:44:37Z",
    "biotoolsCURIE": "biotools:faret",
    "biotoolsID": "faret",
    "confidence_flag": "tool",
    "credit": [
        {
            "name": "Fabian A. Soto",
            "typeEntity": "Person"
        }
    ],
    "description": "FaReT (Face Research Toolkit) is a free and open-source toolkit of three-dimensional models and software to study face perception.",
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Visualisation",
                    "uri": "http://edamontology.org/operation_0337"
                },
                {
                    "term": "Validation",
                    "uri": "http://edamontology.org/operation_2428"
                },
                {
                    "term": "Standardisation and normalisation",
                    "uri": "http://edamontology.org/operation_3435"
                }
            ]
        }
    ],
    "homepage": "https://github.com/fsotoc/FaReT",
    "language": [
        "Python"
    ],
    "lastUpdate": "2021-03-10T19:58:19Z",
    "license": "GPL-3.0",
    "name": "FaReT",
    "owner": "Kigaard",
    "publication": [
        {
            "doi": "10.3758/S13428-020-01421-4",
            "metadata": {
                "abstract": "\u00a9 2020, The Psychonomic Society, Inc.A problem in the study of face perception is that results can be confounded by poor stimulus control. Ideally, experiments should precisely manipulate facial features under study and tightly control irrelevant features. Software for 3D face modeling provides such control, but there is a lack of free and open source alternatives specifically created for face perception research. Here, we provide such tools by expanding the open-source software MakeHuman. We present a database of 27 identity models and six expression pose models (sadness, anger, happiness, disgust, fear, and surprise), together with software to manipulate the models in ways that are common in the face perception literature, allowing researchers to: (1) create a sequence of renders from interpolations between two or more 3D models (differing in identity, expression, and/or pose), resulting in a \u201cmorphing\u201d sequence; (2) create renders by extrapolation in a direction of face space, obtaining 3D \u201canti-faces\u201d and caricatures; (3) obtain videos of dynamic faces from rendered images; (4) obtain average face models; (5) standardize a set of models so that they differ only in selected facial shape features, and (6) communicate with experiment software (e.g., PsychoPy) to render faces dynamically online. These tools vastly improve both the speed at which face stimuli can be produced and the level of control that researchers have over face stimuli. We validate the face model database and software tools through a small study on human perceptual judgments of stimuli produced with the toolkit.",
                "authors": [
                    {
                        "name": "Hays J."
                    },
                    {
                        "name": "Wong C."
                    },
                    {
                        "name": "Soto F.A."
                    }
                ],
                "date": "2020-12-01T00:00:00Z",
                "journal": "Behavior Research Methods",
                "title": "FaReT: A free and open-source toolkit of three-dimensional models and software to study face perception"
            },
            "pmid": "32519291"
        }
    ],
    "topic": [
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        },
        {
            "term": "Gene expression",
            "uri": "http://edamontology.org/topic_0203"
        },
        {
            "term": "Literature and language",
            "uri": "http://edamontology.org/topic_3068"
        }
    ]
}