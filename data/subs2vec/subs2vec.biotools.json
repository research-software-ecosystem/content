{
    "additionDate": "2021-01-18T10:27:26Z",
    "biotoolsCURIE": "biotools:subs2vec",
    "biotoolsID": "subs2vec",
    "confidence_flag": "tool",
    "credit": [
        {
            "name": "Jeroen van Paridon",
            "typeEntity": "Person"
        }
    ],
    "description": "Word embeddings from subtitles in 55 languages.\n\nVan Paridon & Thompson (2019) introduces pretrained embeddings and precomputed word/bigram/trigram frequencies in 55 languages. The files can be downloaded from the links in this table. Word vectors trained on subtitles are available, as well as vectors trained on Wikipedia, and a combination of subtitles and Wikipedia (for best predictive performance).\n\nThis repository contains the subs2vec module, a number of Python 3.7 scripts and command line tools to evaluate a set of word vectors on semantic similarity, semantic and syntactic analogy, and lexical norm prediction tasks. In addition, the subs2vec.py script will take an OpenSubtitles archive or Wikipedia and go through all the steps to train a fastText model and produce word vectors as used in the paper associated with this repository.",
    "editPermission": {
        "type": "private"
    },
    "function": [
        {
            "operation": [
                {
                    "term": "Parsing",
                    "uri": "http://edamontology.org/operation_1812"
                }
            ]
        }
    ],
    "homepage": "https://github.com/jvparidon/subs2vec",
    "language": [
        "Python"
    ],
    "lastUpdate": "2021-02-24T12:11:41Z",
    "license": "MIT",
    "name": "subs2vec",
    "owner": "zsmag19",
    "publication": [
        {
            "doi": "10.3758/S13428-020-01406-3",
            "metadata": {
                "abstract": "Â© 2020, The Author(s).This paper introduces a novel collection of word embeddings, numerical representations of lexical semantics, in 55 languages, trained on a large corpus of pseudo-conversational speech transcriptions from television shows and movies. The embeddings were trained on the OpenSubtitles corpus using the fastText implementation of the skipgram algorithm. Performance comparable with (and in some cases exceeding) embeddings trained on non-conversational (Wikipedia) text is reported on standard benchmark evaluation datasets. A novel evaluation method of particular relevance to psycholinguists is also introduced: prediction of experimental lexical norms in multiple languages. The models, as well as code for reproducing the models and all analyses reported in this paper (implemented as a user-friendly Python package), are freely available at: https://github.com/jvparidon/subs2vec.",
                "authors": [
                    {
                        "name": "Thompson B."
                    },
                    {
                        "name": "van Paridon J."
                    }
                ],
                "date": "2021-04-01T00:00:00Z",
                "journal": "Behavior Research Methods",
                "title": "subs2vec: Word embeddings from subtitles in 55 languages"
            },
            "pmid": "32789660"
        }
    ],
    "topic": [
        {
            "term": "Gene expression",
            "uri": "http://edamontology.org/topic_0203"
        },
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        },
        {
            "term": "Proteomics experiment",
            "uri": "http://edamontology.org/topic_3520"
        }
    ]
}
