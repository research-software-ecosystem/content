{
  "@context": {
    "biotools": "https://bio.tools/ontology/",
    "bsc": "http://bioschemas.org/",
    "bsct": "http://bioschemas.org/types/",
    "dct": "http://purl.org/dc/terms/",
    "edam": "http://edamontology.org/",
    "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
    "sc": "http://schema.org/",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
  },
  "@graph": [
    {
      "@id": "https://bio.tools/subs2vec",
      "@type": "sc:SoftwareApplication",
      "dct:conformsTo": "https://bioschemas.org/profiles/ComputationalTool/0.6-DRAFT",
      "sc:applicationSubCategory": [
        {
          "@id": "edam:topic_3520"
        },
        {
          "@id": "edam:topic_3382"
        },
        {
          "@id": "edam:topic_0203"
        }
      ],
      "sc:citation": [
        "pubmed:32789660",
        {
          "@id": "https://doi.org/10.3758/S13428-020-01406-3"
        }
      ],
      "sc:description": "Word embeddings from subtitles in 55 languages.\n\nVan Paridon & Thompson (2019) introduces pretrained embeddings and precomputed word/bigram/trigram frequencies in 55 languages. The files can be downloaded from the links in this table. Word vectors trained on subtitles are available, as well as vectors trained on Wikipedia, and a combination of subtitles and Wikipedia (for best predictive performance).\n\nThis repository contains the subs2vec module, a number of Python 3.7 scripts and command line tools to evaluate a set of word vectors on semantic similarity, semantic and syntactic analogy, and lexical norm prediction tasks. In addition, the subs2vec.py script will take an OpenSubtitles archive or Wikipedia and go through all the steps to train a fastText model and produce word vectors as used in the paper associated with this repository.",
      "sc:featureList": {
        "@id": "edam:operation_1812"
      },
      "sc:license": "MIT",
      "sc:name": "subs2vec",
      "sc:url": "https://github.com/jvparidon/subs2vec"
    },
    {
      "@id": "https://doi.org/10.3758/S13428-020-01406-3",
      "@type": "sc:CreativeWork"
    }
  ]
}