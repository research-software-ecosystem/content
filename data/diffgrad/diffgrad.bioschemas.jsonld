{
  "@context": {
    "biotools": "https://bio.tools/ontology/",
    "bsc": "http://bioschemas.org/",
    "bsct": "http://bioschemas.org/types/",
    "dct": "http://purl.org/dc/terms/",
    "edam": "http://edamontology.org/",
    "rdf": "http://www.w3.org/1999/02/22-rdf-syntax-ns#",
    "rdfs": "http://www.w3.org/2000/01/rdf-schema#",
    "sc": "http://schema.org/",
    "xsd": "http://www.w3.org/2001/XMLSchema#"
  },
  "@graph": [
    {
      "@id": "https://bio.tools/diffGrad",
      "@type": "sc:SoftwareApplication",
      "dct:conformsTo": "https://bioschemas.org/profiles/ComputationalTool/0.6-DRAFT",
      "sc:applicationSubCategory": [
        {
          "@id": "edam:topic_3382"
        },
        {
          "@id": "edam:topic_3474"
        },
        {
          "@id": "edam:topic_3315"
        }
      ],
      "sc:citation": [
        "pubmed:31880565",
        {
          "@id": "https://doi.org/10.1109/TNNLS.2019.2955777"
        }
      ],
      "sc:description": "An Optimization Method for Convolutional Neural Networks.\n\nAbstract Stochastic Gradient Decent (SGD) is one of the core techniques behind the success of deep neural networks",
      "sc:license": "MIT",
      "sc:name": "diffGrad",
      "sc:url": "https://github.com/shivram1987/diffGrad"
    },
    {
      "@id": "https://doi.org/10.1109/TNNLS.2019.2955777",
      "@type": "sc:CreativeWork"
    }
  ]
}