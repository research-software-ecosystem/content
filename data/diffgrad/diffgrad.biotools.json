{
    "additionDate": "2020-01-14T19:32:19Z",
    "biotoolsCURIE": "biotools:diffGrad",
    "biotoolsID": "diffGrad",
    "confidence_flag": "tool",
    "description": "An Optimization Method for Convolutional Neural Networks.\n\nAbstract Stochastic Gradient Decent (SGD) is one of the core techniques behind the success of deep neural networks",
    "editPermission": {
        "type": "public"
    },
    "homepage": "https://github.com/shivram1987/diffGrad",
    "language": [
        "Python"
    ],
    "lastUpdate": "2020-12-22T07:10:47.278259Z",
    "license": "MIT",
    "link": [
        {
            "type": [
                "Issue tracker"
            ],
            "url": "https://github.com/shivram1987/diffGrad/issues"
        }
    ],
    "name": "diffGrad",
    "owner": "Pub2Tools",
    "publication": [
        {
            "doi": "10.1109/TNNLS.2019.2955777",
            "pmid": "31880565"
        }
    ],
    "topic": [
        {
            "term": "Machine learning",
            "uri": "http://edamontology.org/topic_3474"
        },
        {
            "term": "Imaging",
            "uri": "http://edamontology.org/topic_3382"
        },
        {
            "term": "Mathematics",
            "uri": "http://edamontology.org/topic_3315"
        }
    ]
}